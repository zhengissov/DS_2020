{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4 (Backprop).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTDiHz80PNEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0 + np.exp(-k*x))\n",
        "\n",
        "#mistake was here\n",
        "def sigmoid_prime(x):\n",
        "    return x*(1.0 - x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1.0 - x**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdo7aRq9PpzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "97b92252-f081-471b-d375-9a727913f115"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        self.activation = sigmoid\n",
        "        self.activation_prime = sigmoid_prime\n",
        "        self.output_errors = []\n",
        "        # self.activation = tanh\n",
        "        # self.activation_prime = tanh_prime\n",
        "\n",
        "        # Set weights\n",
        "        self.weights = []\n",
        "        # layers = [2,2,1]\n",
        "        # range of weight values (-1,1)\n",
        "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
        "        \n",
        "        for i in range(1, len(layers) - 1):\n",
        "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
        "            self.weights.append(r)\n",
        "            print(r)\n",
        "        # output layer - random((2+1, 1)) : 3 x 1\n",
        "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
        "        print(r)\n",
        "        self.weights.append(r)\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
        "        # Add column of ones to X\n",
        "        # This is to add the bias unit to the input layer\n",
        "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
        "        X = np.concatenate((ones.T, X), axis=1)\n",
        "        for k in range(epochs):\n",
        "            i = np.random.randint(X.shape[0])\n",
        "            a = [X[i]]\n",
        "\n",
        "            for l in range(len(self.weights)):\n",
        "                    dot_value = np.dot(a[l], self.weights[l])\n",
        "                    activation = self.activation(dot_value)\n",
        "                    a.append(activation)\n",
        "            # output layer\n",
        "            error = y[i] - a[-1]\n",
        "            #take errors\n",
        "            self.output_errors.append(error)\n",
        "            deltas = [error * self.activation_prime(a[-1])]\n",
        "\n",
        "            # we have to start at the second to last layer \n",
        "            # (a layer before the output layer)\n",
        "            for l in range(len(a) - 2, 0, -1): \n",
        "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
        "\n",
        "            # reverse\n",
        "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "            deltas.reverse()\n",
        "\n",
        "            # backpropagation\n",
        "            # 1. Multiply its output delta and input activation \n",
        "            #    to get the gradient of the weight.\n",
        "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
        "            for i in range(len(self.weights)):\n",
        "                layer = np.atleast_2d(a[i])\n",
        "                delta = np.atleast_2d(deltas[i])\n",
        "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
        "\n",
        "            if k % 10000 == 0: \n",
        "                print('epochs:', k)\n",
        "\n",
        "    def predict(self, x): \n",
        "    \n",
        "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
        "\n",
        "        for l in range(0, len(self.weights)):\n",
        "            a = self.activation(np.dot(a, self.weights[l]))\n",
        "        return a\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,2,1])\n",
        "    X = np.array([[0, 0],\n",
        "                  [0, 1],\n",
        "                  [1, 0],\n",
        "                  [1, 1]])\n",
        "    y = np.array([0, 1, 1, 0])\n",
        "#     X = np.array([[-1, -1],\n",
        "#                   [-1, 1],\n",
        "#                   [1, -1],\n",
        "#                   [1, 1]])\n",
        "#     y = np.array([0, 1, 1, 0])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.53486986  0.88400545  0.97753062]\n",
            " [-0.31313808  0.23322387 -0.8691225 ]\n",
            " [-0.28261924  0.39822835  0.27913636]]\n",
            "[[ 0.90481798]\n",
            " [-0.96280279]\n",
            " [ 0.14440261]]\n",
            "epochs: 0\n",
            "epochs: 10000\n",
            "epochs: 20000\n",
            "epochs: 30000\n",
            "epochs: 40000\n",
            "epochs: 50000\n",
            "epochs: 60000\n",
            "epochs: 70000\n",
            "epochs: 80000\n",
            "epochs: 90000\n",
            "[0 0] [0.01243718]\n",
            "[0 1] [0.98380184]\n",
            "[1 0] [0.98223156]\n",
            "[1 1] [0.01841382]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdtwoFBzPqxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "401aee07-b379-48ab-9bb8-7283ee94fe6a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "x = np.linspace(0,100000,100000)\n",
        "error = nn.output_errors\n",
        "plt.plot(x,error)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f014c70f908>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWU45AMzMcsoBYGaWU/8f7UYed+K47o4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}